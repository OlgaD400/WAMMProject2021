{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week4 (Wuwei Zhang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mnist import MNIST\n",
    "\n",
    "def load_dataset():\n",
    "    mndata = MNIST('C:/Users/Zhang/python-mnist/data')\n",
    "    A_train, labels_train = map(np.array, mndata.load_training()) \n",
    "    A_test, labels_test = map(np.array, mndata.load_testing()) \n",
    "    A_train = A_train/255.0\n",
    "    A_test = A_test/255.0\n",
    "    \n",
    "    return A_train, A_test, labels_train, labels_test\n",
    "\n",
    "A_train, A_test, labels_train, labels_test = load_dataset()\n",
    "\n",
    "\n",
    "B_labels_train = np.zeros((60000, 10))\n",
    "for i in range(len(labels_train)):\n",
    "    B_labels_train[i, labels_train[i]] = 1\n",
    "B_labels_test = np.zeros((10000, 10))\n",
    "for j in range(len(labels_test)):\n",
    "    B_labels_test[j, labels_test[j]] = 1\n",
    "    \n",
    "\n",
    "\n",
    "# Create Binary Classification Dataset   \n",
    "\n",
    "ind2 = np.where(labels_train == 2)[0]\n",
    "ind7 = np.where(labels_train ==7)[0]\n",
    "\n",
    "bin_A_train = np.vstack((A_train[ind2,:], A_train[ind7,:]))\n",
    "bin_label_train = np.hstack((-1*np.ones(len(ind2)), np.ones(len(ind7))))\n",
    "\n",
    "\n",
    "ind2_test = np.where(labels_test == 2)[0]\n",
    "ind7_test = np.where(labels_test ==7)[0]\n",
    "\n",
    "bin_A_test = np.vstack((A_test[ind2_test,:], A_test[ind7_test,:]))\n",
    "bin_label_test = np.hstack((-1*np.ones(len(ind2_test)), np.ones(len(ind7_test))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Minimize J(x,y) using gradient descent\n",
    "\n",
    "# Define err = $\\eta||x_{t+1} - x_t||_2 + \\eta|y_{t+1} - y|$\n",
    "\n",
    "def mu_i(x, y, i):\n",
    "    return 1/(1 + np.exp((-1)*bin_label_train[i]*(y+bin_A_train[i,]@x)))\n",
    "\n",
    "              \n",
    "def J_grad_x(x, y):\n",
    "    s = np.zeros(784)\n",
    "    for i in range(12223):\n",
    "        s1 = (-1)*bin_label_train[i]*(bin_A_train[i,])*(1-mu_i(x, y, i))\n",
    "        s = np.add(s, s1)\n",
    "    return s / 12223 + 0.01 * 2 * x\n",
    "\n",
    "\n",
    "def J_grad_y(x, y):\n",
    "    s = 0\n",
    "    for i in range(12223):\n",
    "        s1 = (-1)*bin_label_train[i]*(1-mu_i(x, y, i))\n",
    "        s = np.add(s, s1)\n",
    "    return s / 12223\n",
    "\n",
    "\n",
    "def J(x, y):\n",
    "    s = 0;\n",
    "    for i in range(12223):\n",
    "        s1 = 1 + np.exp((-1)*bin_label_train[i]*(y+bin_A_train[i,]@x))\n",
    "        s = s + np.log(s1)\n",
    "    return s / 12223 + 0.01 * np.square(x).sum()/784\n",
    "\n",
    "\n",
    "def J_testing(x, y):\n",
    "    s = 0;\n",
    "    for i in range(2060):\n",
    "        s1 = 1 + np.exp((-1)*bin_label_test[i]*(y+bin_A_test[i,]@x))\n",
    "        s = s + np.log(s1)\n",
    "    return s / 2060 + 0.01 * np.square(x).sum()/784\n",
    "\n",
    "\n",
    "def grad_descent(xinit, yinit, J_grad_x, J_grad_y, **kwargs):\n",
    "    tol = kwargs.pop('error', 1e-6)\n",
    "    step_size = kwargs.pop('step_size', .1)\n",
    "    \n",
    "    err = 1\n",
    "    xold = xinit\n",
    "    yold = yinit\n",
    "    \n",
    "    max_iter = 500\n",
    "    iter_count = 0\n",
    "    \n",
    "    J_value = [J(xold, yold)]\n",
    "    J_value_testing = [J_testing(xold, yold)]\n",
    "    \n",
    "    while err > tol:\n",
    "        xnew = xold - step_size*J_grad_x(xold, yold)\n",
    "        ynew = yold - step_size*J_grad_y(xnew, yold)\n",
    "        \n",
    "        err1 = step_size * np.linalg.norm(xnew - xold,2)\n",
    "        err2 = step_size * abs(ynew - yold)\n",
    "        err = err1+err2\n",
    "        \n",
    "        J_value.append(J(xnew, ynew))\n",
    "        J_value_testing.append(J_testing(xnew, ynew))\n",
    "        \n",
    "        iter_count+=1\n",
    "        if iter_count>=max_iter:\n",
    "            print('Reached max number of iterations')\n",
    "            return xnew, ynew, J_value, J_value_testing\n",
    "        \n",
    "    return xnew, ynew, J_value, J_value_testing\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "xfinal, yfinal, J_value_train, J_value_testing = grad_descent(np.zeros(784).T, 0, \n",
    "                                                 J_grad_x, J_grad_y, step_size = 1e-3) \n",
    "\n",
    "\n",
    "plt.plot(range(501), J_value_train, label=\"J value for training\") \n",
    "plt.plot(range(501), J_value_testing, label=\"J value for testing\") \n",
    "plt.xlabel('J value')\n",
    "plt.ylabel('Iteration')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Computer Classification Error\n",
    "\n",
    "def clasError(bin_A, bin_label, num):\n",
    "    countError = 0\n",
    "    for i in range(num):\n",
    "        predict = bin_label[i] + bin_A[i, ]@xfinal\n",
    "        if (predict < 0 and bin_label[i] > 0):\n",
    "            countError = countError + 1\n",
    "        elif (predict > 0 and bin_label[i] < 0):\n",
    "            countError = countError + 1\n",
    "            \n",
    "    return countError/num\n",
    "    \n",
    "    \n",
    "print(clasError(bin_A_train, bin_label_train, 12223))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
