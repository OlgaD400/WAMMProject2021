{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week6&7&8 (Wuwei Zhang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mnist import MNIST\n",
    "import torch\n",
    "\n",
    "def load_dataset():\n",
    "    mndata = MNIST('C:/Users/Zhang/python-mnist/data')\n",
    "    A_train, labels_train = map(np.array, mndata.load_training()) \n",
    "    A_test, labels_test = map(np.array, mndata.load_testing()) \n",
    "    A_train = A_train/255.0\n",
    "    A_test = A_test/255.0\n",
    "    \n",
    "    return A_train, A_test, labels_train, labels_test\n",
    "\n",
    "A_train, A_test, labels_train, labels_test = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% #Turn all data into torch tensors\n",
    "\n",
    "A_train = torch.from_numpy(A_train).float()\n",
    "A_test = torch.from_numpy(A_test).float()\n",
    "\n",
    "labels_train_ = torch.from_numpy(labels_train).float().type(torch.LongTensor)\n",
    "labels_test_ = torch.from_numpy(labels_test).float().type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow but wide NN (few layers,many weights)\n",
    "* F1(x) = W1*sigma(W0x + b0) + b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9707\n"
     ]
    }
   ],
   "source": [
    "d = 784 # input, 784 pixels in an image\n",
    "h = 64 # the number of hidden units\n",
    "k = 10 # output, 10 different labels\n",
    "\n",
    "# Initialize weights and biases\n",
    "\n",
    "alpha = 1/np.sqrt(784)\n",
    "W0 = (-alpha - alpha) * torch.rand(h, d) + alpha\n",
    "W0.requires_grad = True #Tells pytorch to keep track of the gradient for these variables\n",
    "W1 = (-alpha - alpha) * torch.rand(k, h) + alpha\n",
    "W1.requires_grad = True\n",
    "b0 = (-alpha - alpha) * torch.rand( h) + alpha\n",
    "b0.requires_grad = True\n",
    "b1 = (-alpha - alpha) * torch.rand(k) + alpha\n",
    "b1.requires_grad = True\n",
    "    \n",
    "# Adam is a more advanced version of gradient descent. \n",
    "# We will use cross entropy with respect to weight for our loss function, \n",
    "# and ReLU for our non-linearity.\n",
    "\n",
    "#Define optimizer \n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam([W0, W1, b0, b1], lr = learning_rate)\n",
    "    \n",
    "#activation function\n",
    "activation = torch.nn.ReLU()\n",
    "    \n",
    "target_accuracy = 0.99\n",
    "accuracy = 0\n",
    "while accuracy<target_accuracy:\n",
    "        \n",
    "    #clears all gradients from the previous step, so you don't accumulate them\n",
    "    optimizer.zero_grad()\n",
    "        \n",
    "    #NN layers to be put here\n",
    "    output = W1@activation(W0@A_train.T+b0[:,None])+b1[:,None]\n",
    "    \n",
    "    Pred_labels = torch.argmax(output.T, axis = 1)\n",
    "    accuracy =( 60000- np.count_nonzero(Pred_labels - labels_train_))/60000\n",
    "    \n",
    "        \n",
    "    # cross entropy loss takes the probabilities generated by the softmax \n",
    "    # function and measures the distance from the truth values.\n",
    "    loss = torch.nn.functional.cross_entropy(output.T, labels_train_)\n",
    "        \n",
    "    #Computes derivatives of the loss with respect to W\n",
    "    loss.backward()\n",
    "        \n",
    "    #optimizer takes a step based on the gradients of the parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "        \n",
    "    \n",
    "# Use test set to test final weights\n",
    "test_output =  W1@activation(W0@A_test.T+b0[:,None])+b1[:,None]\n",
    "test_Pred_labels = torch.argmax(test_output.T, axis = 1)\n",
    "test_accuracy =(10000- np.count_nonzero(test_Pred_labels - labels_test_))/10000\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## narrow but deeper network (many layers, few weights)\n",
    "* F2(x) = W2*sigma(W1*sigma(W0x + b0) + b1) + b2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = 784 # input, 784 pixels in an image\n",
    "h0 = 32 # the number of first hidden units\n",
    "h1 = 32 # the number of second hidden units\n",
    "k = 10 # output, 10 different labels  \n",
    " \n",
    "# Initialize weights and biases\n",
    "\n",
    "alpha = 1/np.sqrt(784)\n",
    "W0 = (-alpha - alpha) * torch.rand(h0, d) + alpha\n",
    "W0.requires_grad = True #Tells pytorch to keep track of the gradient for these variables\n",
    "W1 = (-alpha - alpha) * torch.rand(h1, h0) + alpha\n",
    "W1.requires_grad = True\n",
    "W2 = (-alpha - alpha) * torch.rand(k, h1) + alpha\n",
    "W2.requires_grad = True\n",
    "b0 = (-alpha - alpha) * torch.rand(h0) + alpha\n",
    "b0.requires_grad = True\n",
    "b1 = (-alpha - alpha) * torch.rand(h1) + alpha\n",
    "b1.requires_grad = True\n",
    "b2 = (-alpha - alpha) * torch.rand(k) + alpha\n",
    "b2.requires_grad = True\n",
    "\n",
    "#Define optimizer   \n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam([W0, W1, W2, b0, b1, b2], lr = learning_rate)\n",
    "\n",
    "#activation function\n",
    "activation = torch.nn.ReLU()\n",
    "\n",
    "target_accuracy = 0.99\n",
    "accuracy = 0\n",
    "record_loss = []\n",
    "while accuracy<target_accuracy:\n",
    "        \n",
    "    #clears all gradients from the previous step, so you don't accumulate them\n",
    "    optimizer.zero_grad()\n",
    "        \n",
    "    #NN layers to be put here\n",
    "    output = W2@activation(W1@activation(W0@A_train.T+b0[:,None])+b1[:,None]) + b2[:,None]\n",
    "    \n",
    "    Pred_labels = torch.argmax(output.T, axis = 1)\n",
    "    accuracy =( 60000- np.count_nonzero(Pred_labels - labels_train_))/60000\n",
    "    \n",
    "        \n",
    "    # cross entropy loss takes the probabilities generated by the softmax \n",
    "    # function and measures the distance from the truth values.\n",
    "    loss = torch.nn.functional.cross_entropy(output.T, labels_train_)\n",
    "    record_loss.append(loss)\n",
    "        \n",
    "    #Computes derivatives of the loss with respect to W\n",
    "    loss.backward()\n",
    "        \n",
    "    #optimizer takes a step based on the gradients of the parameters\n",
    "    # updates the weights using gradient of loss with respect to the weights\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "# Use test set to test final weights\n",
    "test_output =  W2@activation(W1@activation(W0@A_test.T+b0[:,None])+b1[:,None]) + b2[:,None]\n",
    "test_Pred_labels = torch.argmax(test_output.T, axis = 1)\n",
    "test_accuracy =(10000- np.count_nonzero(test_Pred_labels - labels_test_))/10000\n",
    "\n",
    "plt.plot(list(range(len(record_loss))), record_loss, label=\"loss\")\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
